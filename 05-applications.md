# ç¬¬5ç« ï¼šCLIPåº”ç”¨åœºæ™¯å’Œç©æ³•

## 5.1 å›¾åƒæ£€ç´¢ç³»ç»Ÿ

### 5.1.1 æ„å»ºå¤§è§„æ¨¡å›¾åƒæ£€ç´¢ç³»ç»Ÿ

```python
import torch
import clip
import faiss
import numpy as np
from PIL import Image
import redis
import json
from pathlib import Path
from typing import List, Dict, Tuple
import hashlib

class ProductImageSearch:
    """ç”µå•†äº§å“å›¾åƒæœç´¢ç³»ç»Ÿ"""
    
    def __init__(self, model_name="ViT-L/14", redis_host="localhost", redis_port=6379):
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.model, self.preprocess = clip.load(model_name, device=self.device)
        
        # ä½¿ç”¨FAISSæ„å»ºå‘é‡ç´¢å¼•
        self.dimension = 768  # ViT-L/14çš„ç‰¹å¾ç»´åº¦
        self.index = faiss.IndexFlatIP(self.dimension)
        
        # ä½¿ç”¨Rediså­˜å‚¨å…ƒæ•°æ®
        self.redis_client = redis.Redis(host=redis_host, port=redis_port, decode_responses=True)
        
        # äº§å“æ•°æ®
        self.product_ids = []
        
    def add_product(self, product_id: str, image_path: str, metadata: Dict):
        """æ·»åŠ äº§å“åˆ°æœç´¢ç³»ç»Ÿ"""
        # æå–å›¾åƒç‰¹å¾
        image = Image.open(image_path).convert('RGB')
        image_tensor = self.preprocess(image).unsqueeze(0).to(self.device)
        
        with torch.no_grad():
            features = self.model.encode_image(image_tensor)
            features = features / features.norm(dim=-1, keepdim=True)
            features = features.cpu().numpy().astype('float32')
        
        # æ·»åŠ åˆ°FAISSç´¢å¼•
        self.index.add(features)
        self.product_ids.append(product_id)
        
        # å­˜å‚¨å…ƒæ•°æ®åˆ°Redis
        metadata['image_path'] = image_path
        self.redis_client.hset(f"product:{product_id}", mapping=metadata)
        
        # å­˜å‚¨å›¾åƒå“ˆå¸Œä»¥æ£€æµ‹é‡å¤
        image_hash = self._compute_image_hash(image_path)
        self.redis_client.set(f"hash:{image_hash}", product_id)
    
    def search_by_text(self, query: str, filters: Dict = None, k: int = 20) -> List[Dict]:
        """ä½¿ç”¨è‡ªç„¶è¯­è¨€æœç´¢äº§å“"""
        # ç¼–ç æŸ¥è¯¢æ–‡æœ¬
        text = clip.tokenize([query]).to(self.device)
        
        with torch.no_grad():
            text_features = self.model.encode_text(text)
            text_features = text_features / text_features.norm(dim=-1, keepdim=True)
            text_features = text_features.cpu().numpy().astype('float32')
        
        # æœç´¢ç›¸ä¼¼äº§å“
        scores, indices = self.index.search(text_features, k * 2)  # è·å–æ›´å¤šç»“æœç”¨äºè¿‡æ»¤
        
        # æ„å»ºç»“æœ
        results = []
        for idx, score in zip(indices[0], scores[0]):
            if idx < len(self.product_ids):
                product_id = self.product_ids[idx]
                metadata = self.redis_client.hgetall(f"product:{product_id}")
                
                # åº”ç”¨è¿‡æ»¤å™¨
                if filters and not self._apply_filters(metadata, filters):
                    continue
                
                result = {
                    'product_id': product_id,
                    'score': float(score),
                    'metadata': metadata
                }
                results.append(result)
                
                if len(results) >= k:
                    break
        
        return results
    
    def search_by_image(self, image_path: str, k: int = 20) -> List[Dict]:
        """ä½¿ç”¨å›¾åƒæœç´¢ç›¸ä¼¼äº§å“"""
        # æ£€æŸ¥æ˜¯å¦ä¸ºé‡å¤å›¾åƒ
        image_hash = self._compute_image_hash(image_path)
        duplicate_id = self.redis_client.get(f"hash:{image_hash}")
        
        if duplicate_id:
            print(f"Found duplicate image: {duplicate_id}")
        
        # æå–å›¾åƒç‰¹å¾
        image = Image.open(image_path).convert('RGB')
        image_tensor = self.preprocess(image).unsqueeze(0).to(self.device)
        
        with torch.no_grad():
            features = self.model.encode_image(image_tensor)
            features = features / features.norm(dim=-1, keepdim=True)
            features = features.cpu().numpy().astype('float32')
        
        # æœç´¢
        scores, indices = self.index.search(features, k)
        
        # æ„å»ºç»“æœ
        results = []
        for idx, score in zip(indices[0], scores[0]):
            if idx < len(self.product_ids):
                product_id = self.product_ids[idx]
                metadata = self.redis_client.hgetall(f"product:{product_id}")
                
                result = {
                    'product_id': product_id,
                    'score': float(score),
                    'metadata': metadata
                }
                results.append(result)
        
        return results
    
    def hybrid_search(self, text_query: str, image_path: str, 
                     text_weight: float = 0.5, k: int = 20) -> List[Dict]:
        """æ··åˆæœç´¢ï¼šç»“åˆæ–‡æœ¬å’Œå›¾åƒ"""
        # è·å–æ–‡æœ¬ç‰¹å¾
        text = clip.tokenize([text_query]).to(self.device)
        
        # è·å–å›¾åƒç‰¹å¾
        image = Image.open(image_path).convert('RGB')
        image_tensor = self.preprocess(image).unsqueeze(0).to(self.device)
        
        with torch.no_grad():
            text_features = self.model.encode_text(text)
            text_features = text_features / text_features.norm(dim=-1, keepdim=True)
            
            image_features = self.model.encode_image(image_tensor)
            image_features = image_features / image_features.norm(dim=-1, keepdim=True)
            
            # åŠ æƒç»„åˆç‰¹å¾
            combined_features = (text_weight * text_features + 
                               (1 - text_weight) * image_features)
            combined_features = combined_features / combined_features.norm(dim=-1, keepdim=True)
            combined_features = combined_features.cpu().numpy().astype('float32')
        
        # æœç´¢
        scores, indices = self.index.search(combined_features, k)
        
        # æ„å»ºç»“æœ
        results = []
        for idx, score in zip(indices[0], scores[0]):
            if idx < len(self.product_ids):
                product_id = self.product_ids[idx]
                metadata = self.redis_client.hgetall(f"product:{product_id}")
                
                result = {
                    'product_id': product_id,
                    'score': float(score),
                    'metadata': metadata
                }
                results.append(result)
        
        return results
    
    def _compute_image_hash(self, image_path: str) -> str:
        """è®¡ç®—å›¾åƒå“ˆå¸Œå€¼"""
        with open(image_path, 'rb') as f:
            return hashlib.md5(f.read()).hexdigest()
    
    def _apply_filters(self, metadata: Dict, filters: Dict) -> bool:
        """åº”ç”¨è¿‡æ»¤æ¡ä»¶"""
        for key, value in filters.items():
            if key not in metadata:
                return False
            if isinstance(value, list):
                if metadata[key] not in value:
                    return False
            elif metadata[key] != value:
                return False
        return True
```

### 5.1.2 ç›¸ä¼¼å›¾åƒå»é‡ç³»ç»Ÿ

```python
class ImageDeduplication:
    """å›¾åƒå»é‡ç³»ç»Ÿ"""
    
    def __init__(self, similarity_threshold=0.95):
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.model, self.preprocess = clip.load("ViT-B/32", device=self.device)
        self.similarity_threshold = similarity_threshold
        self.feature_cache = {}
        
    def find_duplicates(self, image_paths: List[str]) -> List[List[str]]:
        """æŸ¥æ‰¾é‡å¤å›¾åƒç»„"""
        # æå–æ‰€æœ‰å›¾åƒç‰¹å¾
        features = []
        valid_paths = []
        
        for path in image_paths:
            if path in self.feature_cache:
                features.append(self.feature_cache[path])
                valid_paths.append(path)
            else:
                try:
                    feature = self._extract_feature(path)
                    features.append(feature)
                    valid_paths.append(path)
                    self.feature_cache[path] = feature
                except Exception as e:
                    print(f"Error processing {path}: {e}")
                    continue
        
        if not features:
            return []
        
        # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
        features_matrix = torch.stack(features)
        similarity_matrix = features_matrix @ features_matrix.T
        
        # æŸ¥æ‰¾é‡å¤ç»„
        duplicate_groups = []
        processed = set()
        
        for i in range(len(valid_paths)):
            if i in processed:
                continue
            
            # æ‰¾åˆ°æ‰€æœ‰ç›¸ä¼¼çš„å›¾åƒ
            similar_indices = torch.where(
                similarity_matrix[i] >= self.similarity_threshold
            )[0].tolist()
            
            if len(similar_indices) > 1:
                group = [valid_paths[idx] for idx in similar_indices]
                duplicate_groups.append(group)
                processed.update(similar_indices)
        
        return duplicate_groups
    
    def remove_duplicates(self, image_paths: List[str], 
                         keep_strategy: str = 'first') -> List[str]:
        """ç§»é™¤é‡å¤å›¾åƒï¼Œè¿”å›å”¯ä¸€å›¾åƒåˆ—è¡¨"""
        duplicate_groups = self.find_duplicates(image_paths)
        
        # æ ‡è®°è¦ä¿ç•™çš„å›¾åƒ
        to_keep = set(image_paths)
        
        for group in duplicate_groups:
            if keep_strategy == 'first':
                # ä¿ç•™ç¬¬ä¸€ä¸ªï¼Œç§»é™¤å…¶ä»–
                to_remove = group[1:]
            elif keep_strategy == 'largest':
                # ä¿ç•™æœ€å¤§çš„æ–‡ä»¶
                sizes = [(path, Path(path).stat().st_size) for path in group]
                sizes.sort(key=lambda x: x[1], reverse=True)
                to_remove = [path for path, _ in sizes[1:]]
            elif keep_strategy == 'newest':
                # ä¿ç•™æœ€æ–°çš„æ–‡ä»¶
                times = [(path, Path(path).stat().st_mtime) for path in group]
                times.sort(key=lambda x: x[1], reverse=True)
                to_remove = [path for path, _ in times[1:]]
            else:
                raise ValueError(f"Unknown strategy: {keep_strategy}")
            
            to_keep -= set(to_remove)
        
        return list(to_keep)
    
    def _extract_feature(self, image_path: str) -> torch.Tensor:
        """æå–å›¾åƒç‰¹å¾"""
        image = Image.open(image_path).convert('RGB')
        image_tensor = self.preprocess(image).unsqueeze(0).to(self.device)
        
        with torch.no_grad():
            features = self.model.encode_image(image_tensor)
            features = features / features.norm(dim=-1, keepdim=True)
        
        return features.squeeze(0)
```

## 5.2 å†…å®¹ç†è§£å’Œç”Ÿæˆ

### 5.2.1 æ™ºèƒ½å›¾åƒæ ‡æ³¨ç³»ç»Ÿ

```python
class SmartImageTagger:
    """æ™ºèƒ½å›¾åƒæ ‡æ³¨ç³»ç»Ÿ"""
    
    def __init__(self):
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.model, self.preprocess = clip.load("ViT-L/14", device=self.device)
        
        # å¤šå±‚æ¬¡æ ‡ç­¾ä½“ç³»
        self.tag_hierarchy = {
            'style': ['photograph', 'illustration', 'painting', 'sketch', '3D render', 
                     'cartoon', 'anime', 'realistic', 'abstract'],
            'color': ['black and white', 'colorful', 'monochrome', 'vibrant', 
                     'dark', 'bright', 'pastel', 'neon'],
            'composition': ['close-up', 'wide shot', 'portrait', 'landscape', 
                          'aerial view', 'macro', 'panoramic'],
            'mood': ['happy', 'sad', 'peaceful', 'energetic', 'mysterious', 
                    'romantic', 'dramatic', 'serene'],
            'time': ['daytime', 'night', 'sunrise', 'sunset', 'golden hour', 
                    'blue hour', 'twilight'],
            'weather': ['sunny', 'cloudy', 'rainy', 'snowy', 'foggy', 'stormy'],
            'subject': ['person', 'animal', 'nature', 'urban', 'architecture', 
                       'food', 'technology', 'art', 'sports']
        }
        
    def generate_tags(self, image_path: str, threshold: float = 0.2) -> Dict[str, List[Tuple[str, float]]]:
        """ä¸ºå›¾åƒç”Ÿæˆå¤šå±‚æ¬¡æ ‡ç­¾"""
        image = Image.open(image_path).convert('RGB')
        image_tensor = self.preprocess(image).unsqueeze(0).to(self.device)
        
        results = {}
        
        with torch.no_grad():
            image_features = self.model.encode_image(image_tensor)
            image_features = image_features / image_features.norm(dim=-1, keepdim=True)
            
            for category, tags in self.tag_hierarchy.items():
                # ä¸ºæ¯ä¸ªç±»åˆ«åˆ›å»ºæ–‡æœ¬æç¤º
                prompts = [f"a {tag} image" if category != 'subject' 
                          else f"an image of {tag}" for tag in tags]
                
                text = clip.tokenize(prompts).to(self.device)
                text_features = self.model.encode_text(text)
                text_features = text_features / text_features.norm(dim=-1, keepdim=True)
                
                # è®¡ç®—ç›¸ä¼¼åº¦
                similarities = (100.0 * image_features @ text_features.T).softmax(dim=-1)
                
                # æ”¶é›†è¶…è¿‡é˜ˆå€¼çš„æ ‡ç­¾
                category_tags = []
                for i, tag in enumerate(tags):
                    score = similarities[0][i].item()
                    if score >= threshold:
                        category_tags.append((tag, score))
                
                # æŒ‰åˆ†æ•°æ’åº
                category_tags.sort(key=lambda x: x[1], reverse=True)
                results[category] = category_tags
        
        return results
    
    def generate_description(self, image_path: str) -> str:
        """ç”Ÿæˆå›¾åƒçš„è‡ªç„¶è¯­è¨€æè¿°"""
        tags = self.generate_tags(image_path)
        
        description_parts = []
        
        # ä¸»é¢˜
        if tags.get('subject'):
            subject = tags['subject'][0][0]
            description_parts.append(f"This image shows {subject}")
        
        # é£æ ¼
        if tags.get('style'):
            style = tags['style'][0][0]
            description_parts.append(f"in {style} style")
        
        # é¢œè‰²
        if tags.get('color'):
            color = tags['color'][0][0]
            description_parts.append(f"with {color} colors")
        
        # æƒ…ç»ª
        if tags.get('mood'):
            mood = tags['mood'][0][0]
            description_parts.append(f"creating a {mood} mood")
        
        # ç»„åˆæè¿°
        if description_parts:
            description = " ".join(description_parts) + "."
            return description.capitalize()
        else:
            return "An image."
    
    def suggest_keywords(self, image_path: str, num_keywords: int = 10) -> List[str]:
        """ä¸ºSEOæˆ–æœç´¢ä¼˜åŒ–å»ºè®®å…³é”®è¯"""
        tags = self.generate_tags(image_path, threshold=0.1)
        
        # æ”¶é›†æ‰€æœ‰æ ‡ç­¾åŠå…¶åˆ†æ•°
        all_tags = []
        for category, category_tags in tags.items():
            for tag, score in category_tags:
                # æ ¹æ®ç±»åˆ«è°ƒæ•´æƒé‡
                weight = score
                if category == 'subject':
                    weight *= 1.5  # ä¸»é¢˜æ›´é‡è¦
                elif category == 'style':
                    weight *= 1.2
                
                all_tags.append((tag, weight))
        
        # æ’åºå¹¶è¿”å›å‰Nä¸ª
        all_tags.sort(key=lambda x: x[1], reverse=True)
        keywords = [tag for tag, _ in all_tags[:num_keywords]]
        
        return keywords
```

### 5.2.2 åˆ›æ„å†…å®¹ç”ŸæˆåŠ©æ‰‹

```python
class CreativeAssistant:
    """åˆ›æ„å†…å®¹ç”ŸæˆåŠ©æ‰‹"""
    
    def __init__(self):
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.model, self.preprocess = clip.load("ViT-L/14", device=self.device)
        
    def find_metaphors(self, image_path: str) -> List[Dict]:
        """ä¸ºå›¾åƒæ‰¾åˆ°è§†è§‰éšå–»"""
        metaphors = [
            {'concept': 'freedom', 'visual': 'bird flying', 'strength': 0},
            {'concept': 'growth', 'visual': 'tree growing', 'strength': 0},
            {'concept': 'journey', 'visual': 'winding road', 'strength': 0},
            {'concept': 'strength', 'visual': 'mountain', 'strength': 0},
            {'concept': 'peace', 'visual': 'calm water', 'strength': 0},
            {'concept': 'innovation', 'visual': 'light bulb', 'strength': 0},
            {'concept': 'connection', 'visual': 'bridge', 'strength': 0},
            {'concept': 'transformation', 'visual': 'butterfly', 'strength': 0}
        ]
        
        image = Image.open(image_path).convert('RGB')
        image_tensor = self.preprocess(image).unsqueeze(0).to(self.device)
        
        with torch.no_grad():
            image_features = self.model.encode_image(image_tensor)
            image_features = image_features / image_features.norm(dim=-1, keepdim=True)
            
            # æµ‹è¯•æ¯ä¸ªéšå–»
            for metaphor in metaphors:
                text = clip.tokenize([f"an image representing {metaphor['concept']} through {metaphor['visual']}"]).to(self.device)
                text_features = self.model.encode_text(text)
                text_features = text_features / text_features.norm(dim=-1, keepdim=True)
                
                similarity = (image_features @ text_features.T).item()
                metaphor['strength'] = similarity
        
        # æ’åºå¹¶è¿”å›ç›¸å…³çš„éšå–»
        metaphors.sort(key=lambda x: x['strength'], reverse=True)
        return [m for m in metaphors if m['strength'] > 0.2]
    
    def suggest_color_palette(self, image_path: str) -> Dict:
        """å»ºè®®é…è‰²æ–¹æ¡ˆ"""
        color_moods = {
            'warm': ['red', 'orange', 'yellow', 'brown'],
            'cool': ['blue', 'green', 'purple', 'cyan'],
            'neutral': ['gray', 'beige', 'white', 'black'],
            'vibrant': ['bright red', 'electric blue', 'neon green', 'hot pink'],
            'pastel': ['soft pink', 'baby blue', 'mint green', 'lavender'],
            'earth': ['brown', 'green', 'rust', 'sand']
        }
        
        image = Image.open(image_path).convert('RGB')
        image_tensor = self.preprocess(image).unsqueeze(0).to(self.device)
        
        palette_scores = {}
        
        with torch.no_grad():
            image_features = self.model.encode_image(image_tensor)
            image_features = image_features / image_features.norm(dim=-1, keepdim=True)
            
            for palette_name, colors in color_moods.items():
                # æµ‹è¯•é…è‰²æ–¹æ¡ˆ
                prompts = [f"an image with {color} colors" for color in colors]
                text = clip.tokenize(prompts).to(self.device)
                text_features = self.model.encode_text(text)
                text_features = text_features / text_features.norm(dim=-1, keepdim=True)
                
                similarities = image_features @ text_features.T
                avg_similarity = similarities.mean().item()
                palette_scores[palette_name] = avg_similarity
        
        # æ‰¾åˆ°æœ€ä½³é…è‰²æ–¹æ¡ˆ
        best_palette = max(palette_scores, key=palette_scores.get)
        
        return {
            'recommended_palette': best_palette,
            'colors': color_moods[best_palette],
            'confidence': palette_scores[best_palette],
            'all_scores': palette_scores
        }
    
    def generate_social_media_captions(self, image_path: str, platform: str = 'instagram') -> List[str]:
        """ç”Ÿæˆç¤¾äº¤åª’ä½“æ ‡é¢˜"""
        # åˆ†æå›¾åƒ
        image = Image.open(image_path).convert('RGB')
        image_tensor = self.preprocess(image).unsqueeze(0).to(self.device)
        
        # ä¸åŒå¹³å°çš„é£æ ¼
        platform_styles = {
            'instagram': {
                'tone': ['inspirational', 'aesthetic', 'lifestyle'],
                'length': 'medium',
                'hashtags': True,
                'emoji': True
            },
            'twitter': {
                'tone': ['witty', 'informative', 'engaging'],
                'length': 'short',
                'hashtags': True,
                'emoji': False
            },
            'linkedin': {
                'tone': ['professional', 'insightful', 'thoughtful'],
                'length': 'long',
                'hashtags': False,
                'emoji': False
            }
        }
        
        style = platform_styles.get(platform, platform_styles['instagram'])
        
        # æ£€æµ‹å›¾åƒå†…å®¹
        content_types = ['nature', 'urban', 'people', 'food', 'art', 'technology']
        
        with torch.no_grad():
            image_features = self.model.encode_image(image_tensor)
            image_features = image_features / image_features.norm(dim=-1, keepdim=True)
            
            # æ£€æµ‹å†…å®¹ç±»å‹
            prompts = [f"a photo of {content}" for content in content_types]
            text = clip.tokenize(prompts).to(self.device)
            text_features = self.model.encode_text(text)
            text_features = text_features / text_features.norm(dim=-1, keepdim=True)
            
            similarities = (image_features @ text_features.T).squeeze()
            best_content_idx = similarities.argmax().item()
            content_type = content_types[best_content_idx]
        
        # ç”Ÿæˆæ ‡é¢˜æ¨¡æ¿
        captions = []
        
        if content_type == 'nature':
            captions = [
                "Finding peace in nature's embrace ğŸŒ¿",
                "Mother Earth showing off again",
                "This view though... #NatureLover"
            ]
        elif content_type == 'urban':
            captions = [
                "City lights and urban nights âœ¨",
                "Concrete jungle vibes",
                "Where dreams meet skylines #CityLife"
            ]
        elif content_type == 'food':
            captions = [
                "Good food = Good mood ğŸ½ï¸",
                "Feast your eyes on this!",
                "Foodie paradise found #FoodStagram"
            ]
        
        # æ ¹æ®å¹³å°è°ƒæ•´
        if not style['emoji']:
            captions = [cap.replace('ğŸŒ¿', '').replace('âœ¨', '').replace('ğŸ½ï¸', '') for cap in captions]
        if not style['hashtags']:
            captions = [cap.split('#')[0].strip() for cap in captions]
        
        return captions
```

## 5.3 å¤šæ¨¡æ€äº¤äº’åº”ç”¨

### 5.3.1 è§†è§‰å¯¹è¯ç³»ç»Ÿ

```python
class VisualDialogSystem:
    """è§†è§‰å¯¹è¯ç³»ç»Ÿ"""
    
    def __init__(self):
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.model, self.preprocess = clip.load("ViT-L/14", device=self.device)
        self.conversation_history = []
        self.image_context = None
        
    def set_image_context(self, image_path: str):
        """è®¾ç½®å¯¹è¯çš„å›¾åƒä¸Šä¸‹æ–‡"""
        image = Image.open(image_path).convert('RGB')
        image_tensor = self.preprocess(image).unsqueeze(0).to(self.device)
        
        with torch.no_grad():
            self.image_context = self.model.encode_image(image_tensor)
            self.image_context = self.image_context / self.image_context.norm(dim=-1, keepdim=True)
        
        # åˆå§‹åˆ†æ
        self.image_analysis = self._analyze_image()
        self.conversation_history = []
        
    def _analyze_image(self) -> Dict:
        """åˆ†æå›¾åƒåŸºæœ¬ä¿¡æ¯"""
        if self.image_context is None:
            return {}
        
        analyses = {}
        
        # æ£€æµ‹å¯¹è±¡
        objects = ['person', 'animal', 'vehicle', 'building', 'plant', 'food', 'furniture']
        object_prompts = [f"a photo with a {obj}" for obj in objects]
        
        # æ£€æµ‹åœºæ™¯
        scenes = ['indoor', 'outdoor', 'street', 'nature', 'beach', 'mountain', 'city']
        scene_prompts = [f"a photo taken {scene}" if scene in ['indoor', 'outdoor'] 
                        else f"a photo of a {scene}" for scene in scenes]
        
        with torch.no_grad():
            # å¯¹è±¡æ£€æµ‹
            text = clip.tokenize(object_prompts).to(self.device)
            text_features = self.model.encode_text(text)
            text_features = text_features / text_features.norm(dim=-1, keepdim=True)
            
            object_similarities = (self.image_context @ text_features.T).squeeze()
            detected_objects = [obj for obj, sim in zip(objects, object_similarities) if sim > 0.25]
            analyses['objects'] = detected_objects
            
            # åœºæ™¯æ£€æµ‹
            text = clip.tokenize(scene_prompts).to(self.device)
            text_features = self.model.encode_text(text)
            text_features = text_features / text_features.norm(dim=-1, keepdim=True)
            
            scene_similarities = (self.image_context @ text_features.T).squeeze()
            best_scene_idx = scene_similarities.argmax().item()
            analyses['scene'] = scenes[best_scene_idx]
        
        return analyses
    
    def answer_question(self, question: str) -> str:
        """å›ç­”å…³äºå›¾åƒçš„é—®é¢˜"""
        if self.image_context is None:
            return "Please set an image context first."
        
        # è®°å½•é—®é¢˜
        self.conversation_history.append({'type': 'question', 'content': question})
        
        # é—®é¢˜ç±»å‹è¯†åˆ«
        question_lower = question.lower()
        
        # è®¡æ•°é—®é¢˜
        if 'how many' in question_lower:
            return self._answer_counting_question(question)
        
        # æ˜¯å¦é—®é¢˜
        elif question_lower.startswith(('is', 'are', 'does', 'do', 'can')):
            return self._answer_yes_no_question(question)
        
        # ä»€ä¹ˆé—®é¢˜
        elif 'what' in question_lower:
            return self._answer_what_question(question)
        
        # å“ªé‡Œé—®é¢˜
        elif 'where' in question_lower:
            return self._answer_where_question(question)
        
        # æè¿°é—®é¢˜
        else:
            return self._answer_descriptive_question(question)
    
    def _answer_yes_no_question(self, question: str) -> str:
        """å›ç­”æ˜¯/å¦é—®é¢˜"""
        # åˆ›å»ºè‚¯å®šå’Œå¦å®šçš„é™ˆè¿°
        affirmative = question.replace('?', '').replace('Is', 'This is').replace('Are', 'These are')
        negative = question.replace('?', '').replace('Is', 'This is not').replace('Are', 'These are not')
        
        prompts = [affirmative, negative]
        text = clip.tokenize(prompts).to(self.device)
        
        with torch.no_grad():
            text_features = self.model.encode_text(text)
            text_features = text_features / text_features.norm(dim=-1, keepdim=True)
            
            similarities = (self.image_context @ text_features.T).squeeze()
            
        if similarities[0] > similarities[1]:
            answer = "Yes"
        else:
            answer = "No"
        
        # æ·»åŠ ç½®ä¿¡åº¦
        confidence = abs(similarities[0] - similarities[1]).item()
        if confidence < 0.1:
            answer += ", but I'm not very certain"
        elif confidence > 0.3:
            answer += ", definitely"
        
        self.conversation_history.append({'type': 'answer', 'content': answer})
        return answer
    
    def _answer_counting_question(self, question: str) -> str:
        """å›ç­”è®¡æ•°é—®é¢˜"""
        # æå–è¦è®¡æ•°çš„å¯¹è±¡
        import re
        match = re.search(r'how many (\w+)', question.lower())
        if not match:
            return "I couldn't understand what to count."
        
        item = match.group(1)
        
        # æµ‹è¯•ä¸åŒæ•°é‡
        numbers = ['no', 'one', 'two', 'three', 'four', 'five', 'many']
        prompts = [f"a photo with {num} {item}" for num in numbers]
        
        text = clip.tokenize(prompts).to(self.device)
        
        with torch.no_grad():
            text_features = self.model.encode_text(text)
            text_features = text_features / text_features.norm(dim=-1, keepdim=True)
            
            similarities = (self.image_context @ text_features.T).squeeze()
            best_idx = similarities.argmax().item()
        
        answer = f"I can see {numbers[best_idx]} {item} in the image"
        
        self.conversation_history.append({'type': 'answer', 'content': answer})
        return answer
    
    def _answer_what_question(self, question: str) -> str:
        """å›ç­”'ä»€ä¹ˆ'ç±»é—®é¢˜"""
        if 'color' in question.lower():
            colors = ['red', 'blue', 'green', 'yellow', 'orange', 'purple', 'black', 'white', 'brown', 'gray']
            prompts = [f"a predominantly {color} image" for color in colors]
            
            text = clip.tokenize(prompts).to(self.device)
            
            with torch.no_grad():
                text_features = self.model.encode_text(text)
                text_features = text_features / text_features.norm(dim=-1, keepdim=True)
                
                similarities = (self.image_context @ text_features.T).squeeze()
                best_idx = similarities.argmax().item()
            
            answer = f"The dominant color appears to be {colors[best_idx]}"
        
        elif 'doing' in question.lower():
            actions = ['standing', 'sitting', 'walking', 'running', 'eating', 'working', 'playing', 'talking']
            prompts = [f"a photo of someone {action}" for action in actions]
            
            text = clip.tokenize(prompts).to(self.device)
            
            with torch.no_grad():
                text_features = self.model.encode_text(text)
                text_features = text_features / text_features.norm(dim=-1, keepdim=True)
                
                similarities = (self.image_context @ text_features.T).squeeze()
                best_idx = similarities.argmax().item()
            
            answer = f"It appears someone is {actions[best_idx]}"
        
        else:
            # é€šç”¨whaté—®é¢˜
            answer = f"Based on my analysis, the image shows {', '.join(self.image_analysis.get('objects', ['something']))}"
        
        self.conversation_history.append({'type': 'answer', 'content': answer})
        return answer
    
    def _answer_where_question(self, question: str) -> str:
        """å›ç­”'å“ªé‡Œ'ç±»é—®é¢˜"""
        scene = self.image_analysis.get('scene', 'unknown location')
        answer = f"This appears to be {scene}"
        
        self.conversation_history.append({'type': 'answer', 'content': answer})
        return answer
    
    def _answer_descriptive_question(self, question: str) -> str:
        """å›ç­”æè¿°æ€§é—®é¢˜"""
        # åŸºäºå›¾åƒåˆ†æç”Ÿæˆæè¿°
        objects = self.image_analysis.get('objects', [])
        scene = self.image_analysis.get('scene', 'location')
        
        if objects:
            answer = f"I can see {', '.join(objects)} in this {scene} setting"
        else:
            answer = f"This appears to be a {scene} scene"
        
        self.conversation_history.append({'type': 'answer', 'content': answer})
        return answer
```

### 5.3.2 å¢å¼ºç°å®ï¼ˆARï¼‰åº”ç”¨

```python
class CLIPARAssistant:
    """åŸºäºCLIPçš„ARåŠ©æ‰‹"""
    
    def __init__(self):
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.model, self.preprocess = clip.load("ViT-B/16", device=self.device)
        self.object_database = {}
        
    def register_ar_object(self, object_id: str, trigger_descriptions: List[str], 
                          ar_content: Dict):
        """æ³¨å†ŒARå¯¹è±¡å’Œè§¦å‘æ¡ä»¶"""
        # é¢„è®¡ç®—è§¦å‘æè¿°çš„ç‰¹å¾
        text = clip.tokenize(trigger_descriptions).to(self.device)
        
        with torch.no_grad():
            text_features = self.model.encode_text(text)
            text_features = text_features / text_features.norm(dim=-1, keepdim=True)
            # å¹³å‡å¤šä¸ªæè¿°
            trigger_feature = text_features.mean(dim=0)
            trigger_feature = trigger_feature / trigger_feature.norm()
        
        self.object_database[object_id] = {
            'trigger_feature': trigger_feature,
            'descriptions': trigger_descriptions,
            'ar_content': ar_content,
            'threshold': 0.3
        }
    
    def detect_ar_triggers(self, frame: np.ndarray) -> List[Dict]:
        """æ£€æµ‹å½“å‰å¸§ä¸­çš„ARè§¦å‘å™¨"""
        # å°†numpyæ•°ç»„è½¬æ¢ä¸ºPILå›¾åƒ
        image = Image.fromarray(frame)
        image_tensor = self.preprocess(image).unsqueeze(0).to(self.device)
        
        triggered_objects = []
        
        with torch.no_grad():
            image_features = self.model.encode_image(image_tensor)
            image_features = image_features / image_features.norm(dim=-1, keepdim=True)
            
            for object_id, obj_data in self.object_database.items():
                similarity = (image_features @ obj_data['trigger_feature']).item()
                
                if similarity >= obj_data['threshold']:
                    triggered_objects.append({
                        'object_id': object_id,
                        'confidence': similarity,
                        'ar_content': obj_data['ar_content']
                    })
        
        return triggered_objects
    
    def contextual_ar_suggestions(self, frame: np.ndarray) -> List[Dict]:
        """åŸºäºåœºæ™¯ç†è§£çš„ARå»ºè®®"""
        image = Image.fromarray(frame)
        image_tensor = self.preprocess(image).unsqueeze(0).to(self.device)
        
        suggestions = []
        
        # åœºæ™¯ç±»å‹æ£€æµ‹
        scene_types = {
            'restaurant': {
                'prompts': ['a photo of a restaurant', 'dining table with food'],
                'ar_suggestions': ['menu overlay', 'nutritional info', 'reviews']
            },
            'museum': {
                'prompts': ['a photo of a museum', 'artwork on display'],
                'ar_suggestions': ['artwork info', 'audio guide', 'artist biography']
            },
            'street': {
                'prompts': ['a street view', 'urban environment'],
                'ar_suggestions': ['navigation arrows', 'store info', 'traffic updates']
            },
            'nature': {
                'prompts': ['nature scenery', 'outdoor landscape'],
                'ar_suggestions': ['species identification', 'trail maps', 'weather info']
            }
        }
        
        with torch.no_grad():
            image_features = self.model.encode_image(image_tensor)
            image_features = image_features / image_features.norm(dim=-1, keepdim=True)
            
            for scene_name, scene_data in scene_types.items():
                text = clip.tokenize(scene_data['prompts']).to(self.device)
                text_features = self.model.encode_text(text)
                text_features = text_features / text_features.norm(dim=-1, keepdim=True)
                
                similarities = (image_features @ text_features.T).squeeze()
                max_similarity = similarities.max().item()
                
                if max_similarity > 0.25:
                    suggestions.append({
                        'scene_type': scene_name,
                        'confidence': max_similarity,
                        'ar_suggestions': scene_data['ar_suggestions']
                    })
        
        return suggestions
```

## 5.4 åˆ›æ–°åº”ç”¨æ¡ˆä¾‹

### 5.4.1 æ™ºèƒ½ç›¸å†Œç®¡ç†

```python
class SmartPhotoAlbum:
    """æ™ºèƒ½ç›¸å†Œç®¡ç†ç³»ç»Ÿ"""
    
    def __init__(self, album_path: str):
        self.album_path = Path(album_path)
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.model, self.preprocess = clip.load("ViT-B/32", device=self.device)
        self.photo_index = {}
        
    def organize_by_events(self):
        """æ ¹æ®äº‹ä»¶è‡ªåŠ¨ç»„ç»‡ç…§ç‰‡"""
        event_patterns = {
            'wedding': ['wedding dress', 'bride and groom', 'wedding ceremony'],
            'birthday': ['birthday cake', 'candles', 'party', 'celebration'],
            'vacation': ['beach', 'mountain', 'tourist', 'landmark'],
            'graduation': ['graduation cap', 'diploma', 'ceremony'],
            'sports': ['playing sports', 'stadium', 'athletic'],
            'concert': ['stage', 'crowd', 'performance', 'music'],
            'family': ['family gathering', 'group photo', 'home']
        }
        
        photo_events = {}
        
        for photo_path in self.album_path.glob("**/*.jpg"):
            image = Image.open(photo_path).convert('RGB')
            image_tensor = self.preprocess(image).unsqueeze(0).to(self.device)
            
            with torch.no_grad():
                image_features = self.model.encode_image(image_tensor)
                image_features = image_features / image_features.norm(dim=-1, keepdim=True)
                
                best_event = None
                best_score = 0
                
                for event_name, patterns in event_patterns.items():
                    prompts = [f"a photo of {pattern}" for pattern in patterns]
                    text = clip.tokenize(prompts).to(self.device)
                    text_features = self.model.encode_text(text)
                    text_features = text_features / text_features.norm(dim=-1, keepdim=True)
                    
                    similarities = (image_features @ text_features.T).squeeze()
                    avg_similarity = similarities.mean().item()
                    
                    if avg_similarity > best_score:
                        best_score = avg_similarity
                        best_event = event_name
                
                if best_score > 0.25:
                    if best_event not in photo_events:
                        photo_events[best_event] = []
                    photo_events[best_event].append(str(photo_path))
        
        # åˆ›å»ºäº‹ä»¶æ–‡ä»¶å¤¹
        for event_name, photos in photo_events.items():
            event_folder = self.album_path / f"Events/{event_name}"
            event_folder.mkdir(parents=True, exist_ok=True)
            
            for photo in photos:
                # åˆ›å»ºç¬¦å·é“¾æ¥æˆ–å¤åˆ¶æ–‡ä»¶
                dest = event_folder / Path(photo).name
                if not dest.exists():
                    dest.symlink_to(photo)
        
        return photo_events
    
    def create_story_timeline(self) -> List[Dict]:
        """åˆ›å»ºç…§ç‰‡æ•…äº‹æ—¶é—´çº¿"""
        from datetime import datetime
        import exifread
        
        timeline = []
        
        for photo_path in self.album_path.glob("**/*.jpg"):
            # è·å–æ‹æ‘„æ—¶é—´
            with open(photo_path, 'rb') as f:
                tags = exifread.process_file(f)
                date_taken = tags.get('EXIF DateTimeOriginal')
                
                if date_taken:
                    date_taken = datetime.strptime(str(date_taken), '%Y:%m:%d %H:%M:%S')
                else:
                    # ä½¿ç”¨æ–‡ä»¶ä¿®æ”¹æ—¶é—´
                    date_taken = datetime.fromtimestamp(photo_path.stat().st_mtime)
            
            # åˆ†æç…§ç‰‡å†…å®¹
            image = Image.open(photo_path).convert('RGB')
            image_tensor = self.preprocess(image).unsqueeze(0).to(self.device)
            
            # ç”Ÿæˆæè¿°
            with torch.no_grad():
                image_features = self.model.encode_image(image_tensor)
                image_features = image_features / image_features.norm(dim=-1, keepdim=True)
                
                # æ£€æµ‹ä¸»è¦å†…å®¹
                content_prompts = [
                    "a photo of people",
                    "a landscape photo",
                    "a photo of food",
                    "an indoor photo",
                    "an outdoor photo"
                ]
                
                text = clip.tokenize(content_prompts).to(self.device)
                text_features = self.model.encode_text(text)
                text_features = text_features / text_features.norm(dim=-1, keepdim=True)
                
                similarities = (image_features @ text_features.T).squeeze()
                best_idx = similarities.argmax().item()
                description = content_prompts[best_idx].replace("a photo of ", "")
            
            timeline.append({
                'path': str(photo_path),
                'date': date_taken,
                'description': description,
                'year': date_taken.year,
                'month': date_taken.strftime('%B')
            })
        
        # æŒ‰æ—¶é—´æ’åº
        timeline.sort(key=lambda x: x['date'])
        
        return timeline
    
    def find_best_shots(self, num_photos: int = 10) -> List[str]:
        """æ‰¾å‡ºæœ€ä½³ç…§ç‰‡"""
        quality_criteria = [
            "a high quality professional photo",
            "a well-composed photograph",
            "a sharp, in-focus image",
            "good lighting and exposure",
            "aesthetically pleasing composition"
        ]
        
        photo_scores = []
        
        for photo_path in self.album_path.glob("**/*.jpg"):
            image = Image.open(photo_path).convert('RGB')
            image_tensor = self.preprocess(image).unsqueeze(0).to(self.device)
            
            with torch.no_grad():
                image_features = self.model.encode_image(image_tensor)
                image_features = image_features / image_features.norm(dim=-1, keepdim=True)
                
                text = clip.tokenize(quality_criteria).to(self.device)
                text_features = self.model.encode_text(text)
                text_features = text_features / text_features.norm(dim=-1, keepdim=True)
                
                similarities = (image_features @ text_features.T).squeeze()
                quality_score = similarities.mean().item()
                
                photo_scores.append((str(photo_path), quality_score))
        
        # æ’åºå¹¶è¿”å›æœ€ä½³ç…§ç‰‡
        photo_scores.sort(key=lambda x: x[1], reverse=True)
        best_photos = [path for path, _ in photo_scores[:num_photos]]
        
        return best_photos
```

## 5.5 æ€»ç»“

æœ¬ç« ä»‹ç»äº†CLIPçš„å„ç§å®é™…åº”ç”¨å’Œåˆ›æ–°ç©æ³•ï¼ŒåŒ…æ‹¬ï¼š

1. **å›¾åƒæ£€ç´¢ç³»ç»Ÿ**ï¼šæ„å»ºå¤§è§„æ¨¡æœç´¢å¼•æ“ã€å»é‡ç³»ç»Ÿ
2. **å†…å®¹ç†è§£ç”Ÿæˆ**ï¼šæ™ºèƒ½æ ‡æ³¨ã€åˆ›æ„è¾…åŠ©
3. **å¤šæ¨¡æ€äº¤äº’**ï¼šè§†è§‰å¯¹è¯ã€ARåº”ç”¨
4. **åˆ›æ–°åº”ç”¨**ï¼šæ™ºèƒ½ç›¸å†Œã€æ•…äº‹ç”Ÿæˆ

è¿™äº›åº”ç”¨å±•ç¤ºäº†CLIPåœ¨å®é™…åœºæ™¯ä¸­çš„å¼ºå¤§èƒ½åŠ›å’Œçµæ´»æ€§ã€‚

## ä¸‹ä¸€æ­¥

ç»§ç»­é˜…è¯» â†’ [ç¬¬6ç« ï¼šCLIPå¾®è°ƒæ•™ç¨‹](./06-fine-tuning.md)